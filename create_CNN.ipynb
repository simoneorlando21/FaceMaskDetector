{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "homework1_transferLearning_mobile_0.92.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "2uAjHHk2gXg-",
        "outputId": "f8fc930d-1cd4-4e9b-da43-cc8f1d217718",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# Set the seed for random operations. \n",
        "# This let our experiments to be reproducible. \n",
        "SEED = 2112\n",
        "tf.random.set_seed(SEED)  \n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "# Get current working directory\n",
        "cwd = os.getcwd()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qiRpBw_Bgvu-"
      },
      "source": [
        "!unzip '/content/drive/My Drive/challenge/artificial-neural-networks-and-deep-learning-2020.zip'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TMNOZSDrg0GP",
        "outputId": "c5b8cbbe-c084-4a28-d4ff-a1c6eda76289",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        }
      },
      "source": [
        "dataset_dir = os.path.join(cwd, 'MaskDataset')\n",
        "dataset_dir"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/MaskDataset'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pUSW8Ffvw6tk"
      },
      "source": [
        "import json\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "\n",
        "with open(\"./MaskDataset/train_gt.json\", \"r\") as f:\n",
        "  data = json.load(f)\n",
        "\n",
        "os.mkdir(\"./MaskDataset/new_train\") \n",
        "os.mkdir(\"./MaskDataset/new_train/0\")\n",
        "os.mkdir(\"./MaskDataset/new_train/1\")\n",
        "os.mkdir(\"./MaskDataset/new_train/2\")\n",
        "os.mkdir(\"./MaskDataset/new_val\")\n",
        "os.mkdir(\"./MaskDataset/new_val/0\")\n",
        "os.mkdir(\"./MaskDataset/new_val/1\")\n",
        "os.mkdir(\"./MaskDataset/new_val/2\")\n",
        "\n",
        "\n",
        "counter_0 = 0\n",
        "counter_1 = 0\n",
        "counter_2 = 0\n",
        "\n",
        "path_dataset = \"./MaskDataset/training/\" #/content/MaskDataset/training\n",
        "path_new_train = \"./MaskDataset/new_train/\" #/content/MaskDataset/new_train/\n",
        "path_new_val = \"./MaskDataset/new_val/\" #/content/MaskDataset/new_val/\n",
        "\n",
        "for attribute, value in data.items():\n",
        "        if value == 0:\n",
        "            counter_0 += 1 \n",
        "            if (counter_0 < 1400):\n",
        "                shutil.copy(path_dataset + attribute,path_new_train + \"0/\" + attribute)\n",
        "            else:\n",
        "                shutil.copy(path_dataset + attribute,path_new_val + \"0/\" + attribute)\n",
        "        elif value == 1:\n",
        "            counter_1 += 1 \n",
        "            if (counter_1 < 1400):\n",
        "                shutil.copy(path_dataset + attribute,path_new_train + \"1/\" + attribute)\n",
        "            else:\n",
        "                shutil.copy(path_dataset + attribute,path_new_val + \"1/\" + attribute)\n",
        "        else:\n",
        "            counter_2 += 1 \n",
        "            if (counter_2 < 1400):\n",
        "                shutil.copy(path_dataset + attribute,path_new_train + \"2/\" + attribute)\n",
        "            else:\n",
        "                shutil.copy(path_dataset + attribute,path_new_val + \"2/\" + attribute)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Meg8Yn1wg2sG",
        "outputId": "7300a559-98bc-4468-8146-5deb0c70a0df",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "img_h = 512\n",
        "img_w = 512\n",
        "num_classes = 3\n",
        "\n",
        "# Batch size\n",
        "bs = 8\n",
        "train_data_gen = ImageDataGenerator(rescale=1/255., rotation_range=10,\n",
        "                                    width_shift_range=10,\n",
        "                                    height_shift_range=10,\n",
        "                                    zoom_range=0.3,\n",
        "                                    horizontal_flip=True,\n",
        "                                    vertical_flip=True,\n",
        "                                    fill_mode='constant',\n",
        "                                    cval=0,)\n",
        "valid_data_gen = ImageDataGenerator(rescale=1/255.)\n",
        "\n",
        "train_gen=train_data_gen.flow_from_directory(\"./MaskDataset/new_train\",class_mode='categorical',shuffle=True, batch_size=bs, target_size=(img_h,img_w)) \n",
        "valid_gen = valid_data_gen.flow_from_directory(\"./MaskDataset/new_val\", class_mode='categorical', shuffle=True, batch_size=bs, target_size=(img_h,img_w))\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 4197 images belonging to 3 classes.\n",
            "Found 1417 images belonging to 3 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rHRtXcFTg5sc"
      },
      "source": [
        ""
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rbCfchnHg8-Z"
      },
      "source": [
        "#params\n",
        "img_h = 512\n",
        "img_w = 512\n",
        "num_classes = 3\n",
        "\n",
        "# Batch size\n",
        "bs = 8\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oXpVaQqehAM8"
      },
      "source": [
        "# Create Dataset objects\n",
        "\n",
        "# Training\n",
        "train_dataset = tf.data.Dataset.from_generator(lambda: train_gen,\n",
        "                                               output_types=(tf.float32, tf.float32),\n",
        "                                               output_shapes=([None, img_h, img_w, 3], [None, num_classes]))\n",
        "train_dataset = train_dataset.repeat()\n",
        "\n",
        "# Validation\n",
        "valid_dataset = tf.data.Dataset.from_generator(lambda: valid_gen, \n",
        "                                               output_types=(tf.float32, tf.float32),                  \n",
        "                                               output_shapes=([None, img_h, img_w, 3], [None, num_classes]))\n",
        "valid_dataset = valid_dataset.repeat()"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Se8X0TVNhBlQ",
        "outputId": "d625ba29-3899-4a2f-f837-c165934bf0fd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "\n",
        "\n",
        "mb = tf.keras.applications.MobileNet(weights='imagenet', include_top=False, input_shape=(img_h, img_w, 3)) #include_top false perchè tolgo il classifier \n",
        "#weights uso quelli di imagenet che sono i più usati"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet/mobilenet_1_0_224_tf_no_top.h5\n",
            "17227776/17225924 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4cgJ9wXKhJpS"
      },
      "source": [
        "# Create Model\n",
        "# ------------\n",
        "\n",
        "finetuning = True\n",
        "\n",
        "#finetuning true significa che da un certo livello in poi ricomincio ad adattare i pesi, partendo da quelli dati\n",
        "#di solito si fa su modelli molto diversi da quello usato in origine\n",
        "\n",
        "if finetuning:\n",
        "    freeze_until = 15 # layer from which we want to fine-tune\n",
        "    \n",
        "    for layer in mb.layers[:freeze_until]:\n",
        "        layer.trainable = False\n",
        "else:\n",
        "    mb.trainable = False\n",
        "    \n",
        "model = tf.keras.Sequential()\n",
        "model.add(mb)\n",
        "model.add(tf.keras.layers.Flatten())\n",
        "model.add(tf.keras.layers.Dense(units=1024, activation='relu'))\n",
        "model.add(tf.keras.layers.Dense(units=512, activation='relu'))\n",
        "model.add(tf.keras.layers.Dense(units=num_classes, activation='softmax'))"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VZErN9jahgw0"
      },
      "source": [
        "# Optimization params\n",
        "# -------------------\n",
        "\n",
        "# Loss\n",
        "loss = tf.keras.losses.CategoricalCrossentropy()\n",
        "\n",
        "# learning rate\n",
        "lr = 1e-4\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
        "# -------------------\n",
        "\n",
        "# Validation metrics\n",
        "# ------------------\n",
        "\n",
        "metrics = ['accuracy']\n",
        "# ------------------\n",
        "\n",
        "# Compile Model\n",
        "model.compile(optimizer=optimizer, loss=loss, metrics=metrics)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wQY-AYBUhkmb",
        "outputId": "b15577aa-c758-48cf-ea3a-c1d01507a761",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "callbacks = []\n",
        "\n",
        "# Early Stopping\n",
        "early_stop = True\n",
        "if early_stop:\n",
        "    es_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "    callbacks.append(es_callback)\n",
        "\n",
        "model.fit_generator(train_gen, epochs=100, validation_data=valid_gen, callbacks=callbacks)\n",
        "    \n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "525/525 [==============================] - 262s 500ms/step - loss: 0.4411 - accuracy: 0.8087 - val_loss: 0.3166 - val_accuracy: 0.8793\n",
            "Epoch 2/100\n",
            "525/525 [==============================] - 263s 501ms/step - loss: 0.4003 - accuracy: 0.8287 - val_loss: 0.2832 - val_accuracy: 0.8878\n",
            "Epoch 3/100\n",
            "525/525 [==============================] - 262s 500ms/step - loss: 0.3012 - accuracy: 0.8751 - val_loss: 0.2521 - val_accuracy: 0.9054\n",
            "Epoch 4/100\n",
            "525/525 [==============================] - 263s 502ms/step - loss: 0.2640 - accuracy: 0.8921 - val_loss: 0.2114 - val_accuracy: 0.9231\n",
            "Epoch 5/100\n",
            "525/525 [==============================] - 261s 497ms/step - loss: 0.2407 - accuracy: 0.9064 - val_loss: 0.2254 - val_accuracy: 0.9061\n",
            "Epoch 6/100\n",
            "525/525 [==============================] - 260s 495ms/step - loss: 0.2331 - accuracy: 0.9078 - val_loss: 0.3009 - val_accuracy: 0.8864\n",
            "Epoch 7/100\n",
            "525/525 [==============================] - 258s 491ms/step - loss: 0.2037 - accuracy: 0.9190 - val_loss: 0.2587 - val_accuracy: 0.8934\n",
            "Epoch 8/100\n",
            "525/525 [==============================] - 259s 493ms/step - loss: 0.1933 - accuracy: 0.9252 - val_loss: 0.1830 - val_accuracy: 0.9330\n",
            "Epoch 9/100\n",
            "525/525 [==============================] - 259s 492ms/step - loss: 0.1684 - accuracy: 0.9376 - val_loss: 0.2127 - val_accuracy: 0.9266\n",
            "Epoch 10/100\n",
            "525/525 [==============================] - 256s 488ms/step - loss: 0.1605 - accuracy: 0.9440 - val_loss: 0.1978 - val_accuracy: 0.9330\n",
            "Epoch 11/100\n",
            "525/525 [==============================] - 257s 489ms/step - loss: 0.1433 - accuracy: 0.9478 - val_loss: 0.1878 - val_accuracy: 0.9195\n",
            "Epoch 12/100\n",
            "525/525 [==============================] - 256s 487ms/step - loss: 0.1487 - accuracy: 0.9471 - val_loss: 0.2415 - val_accuracy: 0.9266\n",
            "Epoch 13/100\n",
            "525/525 [==============================] - 255s 485ms/step - loss: 0.1544 - accuracy: 0.9423 - val_loss: 0.1904 - val_accuracy: 0.9315\n",
            "Epoch 14/100\n",
            "525/525 [==============================] - 256s 487ms/step - loss: 0.1338 - accuracy: 0.9547 - val_loss: 0.1566 - val_accuracy: 0.9450\n",
            "Epoch 15/100\n",
            "525/525 [==============================] - 256s 488ms/step - loss: 0.1275 - accuracy: 0.9562 - val_loss: 0.1882 - val_accuracy: 0.9435\n",
            "Epoch 16/100\n",
            "525/525 [==============================] - 256s 488ms/step - loss: 0.1317 - accuracy: 0.9543 - val_loss: 0.2540 - val_accuracy: 0.9203\n",
            "Epoch 17/100\n",
            "525/525 [==============================] - 256s 488ms/step - loss: 0.1152 - accuracy: 0.9619 - val_loss: 0.1903 - val_accuracy: 0.9379\n",
            "Epoch 18/100\n",
            "525/525 [==============================] - 256s 488ms/step - loss: 0.1130 - accuracy: 0.9595 - val_loss: 0.2371 - val_accuracy: 0.9280\n",
            "Epoch 19/100\n",
            "525/525 [==============================] - 256s 487ms/step - loss: 0.1104 - accuracy: 0.9621 - val_loss: 0.2223 - val_accuracy: 0.9358\n",
            "Epoch 20/100\n",
            "525/525 [==============================] - 256s 488ms/step - loss: 0.1237 - accuracy: 0.9616 - val_loss: 0.1755 - val_accuracy: 0.9428\n",
            "Epoch 21/100\n",
            "525/525 [==============================] - 256s 488ms/step - loss: 0.1053 - accuracy: 0.9633 - val_loss: 0.2473 - val_accuracy: 0.9210\n",
            "Epoch 22/100\n",
            "525/525 [==============================] - 257s 489ms/step - loss: 0.0945 - accuracy: 0.9659 - val_loss: 0.2039 - val_accuracy: 0.9386\n",
            "Epoch 23/100\n",
            "525/525 [==============================] - 257s 489ms/step - loss: 0.0837 - accuracy: 0.9740 - val_loss: 0.2165 - val_accuracy: 0.9379\n",
            "Epoch 24/100\n",
            "525/525 [==============================] - 256s 488ms/step - loss: 0.0898 - accuracy: 0.9700 - val_loss: 0.2180 - val_accuracy: 0.9252\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f3dd25a05f8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HIO4cOfMl0Va"
      },
      "source": [
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "def create_csv(results, results_dir='./'):\n",
        "\n",
        "    csv_fname = 'results_'\n",
        "    csv_fname += datetime.now().strftime('%b%d_%H-%M-%S') + '.csv'\n",
        "\n",
        "    with open(os.path.join(results_dir, csv_fname), 'w') as f:\n",
        "\n",
        "        f.write('Id,Category\\n')\n",
        "\n",
        "        for key, value in results.items():\n",
        "            f.write(key + ',' + str(value) + '\\n')"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I_y8VubKl3GL"
      },
      "source": [
        "results = {}\n",
        "for i in os.listdir(\"/content/MaskDataset/test\"):\n",
        "  img = tf.keras.preprocessing.image.load_img(\"/content/MaskDataset/test/\"+i, target_size=(512,512), color_mode='rgb')\n",
        "  img_array = tf.keras.preprocessing.image.img_to_array(img)\n",
        "  img_array = img_array/255.0\n",
        "  img_array = tf.expand_dims(img_array,0)\n",
        "  predictions = model.predict(img_array)\n",
        "  score = tf.nn.softmax(predictions[0])\n",
        "  predicted_class_int = np.argmax(predictions[0])\n",
        "  results[i] = predicted_class_int\n",
        "create_csv(results)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d1Ccq57fR-k0"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}